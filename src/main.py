import os
import sys
import json
import glob
import argparse
import datetime
import subprocess
import traceback
from dotenv import load_dotenv
from rich.console import Console
from rich.markdown import Markdown
from rich.table import Table
from PIL import Image
from google.api_core.exceptions import ResourceExhausted
import google.generativeai as genai

import api
import utils
from config import load_config, save_config

def get_response_text(response: genai.types.GenerateContentResponse) -> str:
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ n·ªôi dung text t·ª´ m·ªôt response m·ªôt c√°ch an to√†n."""
    try:
        # Th·ª≠ c√°ch nhanh nh·∫•t tr∆∞·ªõc
        return response.text
    except Exception:
        # N·∫øu th·∫•t b·∫°i, gh√©p c√°c ph·∫ßn text l·∫°i v·ªõi nhau
        text_parts = []
        for part in response.parts:
            if hasattr(part, 'text') and part.text:
                text_parts.append(part.text)
        return "".join(text_parts)

def serialize_history(history):
    serializable = []
    for content in history:
        parts_data = []
        for part in content.parts:
            if not hasattr(part, 'text') and not hasattr(part, 'function_call') and not hasattr(part, 'function_response'):
                continue
            if hasattr(part, 'text') and part.text:
                parts_data.append({"text": part.text})
            elif hasattr(part, 'function_call') and part.function_call:
                parts_data.append({"function_call": {"name": part.function_call.name, "args": dict(part.function_call.args)}})
            elif hasattr(part, 'function_response') and part.function_response:
                response_content = part.function_response.response
                if not isinstance(response_content, dict):
                     response_content = {'result': str(response_content)}
                parts_data.append({"function_response": {"name": part.function_response.name, "response": response_content}})
        if parts_data:
            serializable.append({"role": content.role, "parts": parts_data})
    return serializable

def model_selection_wizard(console: Console, config: dict):
    console.print("[bold green]ƒêang l·∫•y danh s√°ch c√°c model kh·∫£ d·ª•ng...[/bold green]")
    try:
        models = api.get_available_models()
        if not models:
            console.print("[bold red]Kh√¥ng t√¨m th·∫•y model n√†o kh·∫£ d·ª•ng.[/bold red]")
            return
    except Exception as e:
        console.print(f"[bold red]L·ªói khi l·∫•y danh s√°ch model: {e}[/bold red]")
        return
    table = Table(title="Ch·ªçn m·ªôt model ƒë·ªÉ l√†m m·∫∑c ƒë·ªãnh")
    table.add_column("#", style="cyan")
    table.add_column("Model Name", style="magenta")
    stable_models = sorted([m for m in models if 'preview' not in m and 'exp' not in m])
    preview_models = sorted([m for m in models if 'preview' in m or 'exp' in m])
    sorted_models = stable_models + preview_models
    for i, model_name in enumerate(sorted_models):
        table.add_row(str(i + 1), model_name)
    console.print(table)
    while True:
        try:
            choice_str = console.input("Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa model b·∫°n mu·ªën ch·ªçn: ")
            choice = int(choice_str) - 1
            if 0 <= choice < len(sorted_models):
                selected_model = sorted_models[choice]
                config['default_model'] = selected_model
                fallback_list = [selected_model]
                for m in stable_models:
                    if m != selected_model and m not in fallback_list:
                        fallback_list.append(m)
                config['model_fallback_order'] = fallback_list
                save_config(config)
                console.print(f"\n[bold green]‚úÖ ƒê√£ ƒë·∫∑t model m·∫∑c ƒë·ªãnh l√†: [cyan]{selected_model}[/cyan][/bold green]")
                console.print(f"[yellow]Th·ª© t·ª± model d·ª± ph√≤ng ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t.[/yellow]")
                break
            else:
                console.print("[bold red]L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, vui l√≤ng th·ª≠ l·∫°i.[/bold red]")
        except ValueError:
            console.print("[bold red]Vui l√≤ng nh·∫≠p m·ªôt con s·ªë.[/bold red]")
        except (KeyboardInterrupt, EOFError):
            console.print("\n[yellow]ƒê√£ h·ªßy l·ª±a ch·ªçn.[/yellow]")
            break

def run_chat_mode(chat_session, console: Console, config: dict, format_type: str, save_path: str = None):
    console.print("[bold green]ƒê√£ v√†o ch·∫ø ƒë·ªô tr√≤ chuy·ªán. G√µ 'exit' ho·∫∑c 'quit' ƒë·ªÉ tho√°t.[/bold green]")
    fallback_models = config.get("model_fallback_order", [])
    current_model_name = chat_session.model.model_name
    current_model_index = fallback_models.index(current_model_name) if current_model_name in fallback_models else -1
    try:
        while True:
            prompt = console.input("\n[bold cyan]You:[/bold cyan] ")
            if prompt.lower().strip() in ["exit", "quit", "q"]: break
            if not prompt.strip(): continue
            with console.status("[bold green]AI ƒëang suy nghƒ©...[/bold green]"):
                try:
                    response = api.send_message(chat_session, [prompt])
                except ResourceExhausted:
                    console.print(f"[bold yellow]‚ö†Ô∏è Model '{chat_session.model.model_name}' ƒë√£ h·∫øt h·∫°n ng·∫°ch.[/bold yellow]")
                    if current_model_index != -1: current_model_index += 1
                    if current_model_index != -1 and current_model_index < len(fallback_models):
                        new_model = fallback_models[current_model_index]
                        console.print(f"[cyan]ƒêang t·ª± ƒë·ªông chuy·ªÉn sang model: '{new_model}'...[/cyan]")
                        history = chat_session.history
                        system_instruction = chat_session.model.system_instruction
                        chat_session = api.start_chat_session(new_model, system_instruction, history)
                        try:
                             response = api.send_message(chat_session, [prompt])
                        except Exception as e:
                            console.print(f"[bold red]L·ªói ngay c·∫£ v·ªõi model d·ª± ph√≤ng: {e}[/bold red]")
                            continue
                    else:
                        console.print("[bold red]‚ùå ƒê√£ th·ª≠ h·∫øt c√°c model d·ª± ph√≤ng nh∆∞ng ƒë·ªÅu th·∫•t b·∫°i.[/bold red]")
                        continue
                except Exception as e:
                    console.print(f"[bold red]L·ªói: {e}[/bold red]")
                    continue
            console.print("\n[bold magenta]AI:[/bold magenta]")
            response_text = get_response_text(response)
            if format_type == 'rich': console.print(Markdown(response_text))
            else: console.print(response_text)
            if response.usage_metadata:
                tokens = response.usage_metadata.total_token_count
                console.print(f"\n[dim]Token usage: {tokens}[/dim]")
            utils.execute_suggested_commands(response_text, console)
    except KeyboardInterrupt:
        console.print("\n[yellow]ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng.[/yellow]")
    finally:
        if not save_path:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"chat_history_{timestamp}.json"
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(serialize_history(chat_session.history), f, indent=2, ensure_ascii=False)
            console.print(f"\n[bold yellow]L·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{save_path}'.[/bold yellow]")
        except Exception as e:
            console.print(f"[bold red]L·ªói khi l∆∞u l·ªãch s·ª≠: {e}[/bold red]")

def show_history_browser(console: Console):
    console.print("[bold green]ƒêang t√¨m ki·∫øm c√°c file l·ªãch s·ª≠ tr√≤ chuy·ªán...[/bold green]")
    history_files = glob.glob("chat_history_*.json")
    if not history_files:
        console.print("[yellow]Kh√¥ng t√¨m th·∫•y file l·ªãch s·ª≠ n√†o.[/yellow]")
        return
    table = Table(title="üìö L·ªãch s·ª≠ Tr√≤ chuy·ªán")
    table.add_column("#", style="cyan")
    table.add_column("File Name", style="magenta")
    table.add_column("Last Modified", style="green")
    history_files.sort(key=os.path.getmtime, reverse=True)
    for i, file_path in enumerate(history_files):
        mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')
        table.add_row(str(i + 1), file_path, mod_time)
    console.print(table)
    try:
        choice_str = console.input("Nh·∫≠p s·ªë ƒë·ªÉ ch·ªçn l·ªãch s·ª≠ mu·ªën ti·∫øp t·ª•c (nh·∫•n Enter ƒë·ªÉ tho√°t): ")
        if not choice_str:
             console.print("[yellow]ƒê√£ tho√°t tr√¨nh duy·ªát l·ªãch s·ª≠.[/yellow]")
             return
        choice = int(choice_str)
        if 1 <= choice <= len(history_files):
            selected_file = history_files[choice - 1]
            console.print("\nƒê·ªÉ ti·∫øp t·ª•c cu·ªôc tr√≤ chuy·ªán n√†y, h√£y ch·∫°y l·ªánh sau:")
            console.print(f'[bold cyan]python src/main.py --chat --load "{selected_file}"[/bold cyan]')
        else:
            console.print("[yellow]L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.[/yellow]")
    except (ValueError, KeyboardInterrupt):
        console.print("[yellow]ƒê√£ tho√°t tr√¨nh duy·ªát l·ªãch s·ª≠.[/yellow]")

def handle_git_commit(chat_session, console: Console, format_type: str):
    try:
        diff = subprocess.check_output(["git", "diff", "--staged"], text=True, encoding='utf-8')
        if not diff:
            console.print("[yellow]Kh√¥ng c√≥ thay ƒë·ªïi n√†o ƒë∆∞·ª£c staged. H√£y d√πng 'git add' tr∆∞·ªõc.[/yellow]")
            return
        prompt = ("D·ª±a tr√™n n·ªôi dung 'git diff' d∆∞·ªõi ƒë√¢y, h√£y vi·∫øt m·ªôt commit message s√∫c t√≠ch v√† √Ω nghƒ©a "
                  "theo chu·∫©n Conventional Commits. Ch·ªâ tr·∫£ v·ªÅ message, kh√¥ng gi·∫£i th√≠ch g√¨ th√™m.\n\n"
                  f"```diff\n{diff}\n```")
        with console.status("[bold green]AI ƒëang t·∫°o commit message...[/bold green]"):
            response = api.send_message(chat_session, [prompt])
        console.print("\nüí° [bold green]Commit message ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:[/bold green]")
        clean_text = get_response_text(response).strip().replace("```", "")
        if format_type == 'rich': console.print(Markdown(clean_text))
        else: console.print(clean_text)
    except FileNotFoundError:
        console.print("[bold red]L·ªói: L·ªánh 'git' kh√¥ng t·ªìn t·∫°i. B·∫°n ƒë√£ c√†i Git ch∆∞a?[/bold red]")
    except subprocess.CalledProcessError:
        console.print("[bold red]L·ªói: ƒê√¢y kh√¥ng ph·∫£i l√† m·ªôt Git repository ho·∫∑c c√≥ l·ªói khi ch·∫°y 'git diff'.[/bold red]")

def handle_code_helper(args, config, console: Console):
    file_path = args.document or args.refactor
    task = "vi·∫øt t√†i li·ªáu (docstrings, comments)" if args.document else "ph√¢n t√≠ch v√† ƒë·ªÅ xu·∫•t c√°c ph∆∞∆°ng √°n t√°i c·∫•u tr√∫c (refactor)"
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            code_content = f.read()
    except FileNotFoundError:
        console.print(f"[bold red]L·ªói: Kh√¥ng t√¨m th·∫•y file '{file_path}'[/bold red]")
        return
    except Exception as e:
        console.print(f"[bold red]L·ªói khi ƒë·ªçc file: {e}[/bold red]")
        return
    system_instruction = "You are an expert software architect specializing in code quality."
    model_name = config.get("default_model") 
    chat_session = api.start_chat_session(model_name, system_instruction)
    prompt = (f"V·ªõi vai tr√≤ l√† m·ªôt ki·∫øn tr√∫c s∆∞ ph·∫ßn m·ªÅm, h√£y {task} cho ƒëo·∫°n m√£ trong file `{file_path}` d∆∞·ªõi ƒë√¢y.\n"
              "Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, chuy√™n nghi·ªáp.\n\n"
              f"```\n{code_content}\n```")
    console.print(f"ü§ñ [bold cyan]ƒêang {task} cho file '{file_path}'...[/bold cyan]")
    with console.status("[bold green]AI ƒëang ph√¢n t√≠ch...[/bold green]"):
        try:
            response = api.send_message(chat_session, [prompt])
        except Exception as e:
            console.print(f"[bold red]L·ªói: {e}[/bold red]")
            return
    console.print("\nüí° [bold green]Ph√¢n t√≠ch & ƒê·ªÅ xu·∫•t:[/bold green]")
    response_text = get_response_text(response)
    console.print(Markdown(response_text))
    if response.usage_metadata:
        tokens = response.usage_metadata.total_token_count
        console.print(f"\n[dim]Token usage: {tokens}[/dim]")

def main():
    os.environ['GRPC_ENABLE_FORK_SUPPORT'] = '0'
    os.environ['GRPC_POLL_STRATEGY'] = 'poll'
    
    load_dotenv()
    console = Console()
    config = load_config()
    
    parser = argparse.ArgumentParser(description="AI Agent CLI m·∫°nh m·∫Ω v·ªõi Gemini.", formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("prompt", nargs='?', default=None, help="C√¢u l·ªánh h·ªèi AI.")
    parser.add_argument("--list-models", action="store_true", help="Li·ªát k√™ models.")
    parser.add_argument("--set-model", action="store_true", help="Ch·∫°y giao di·ªán ƒë·ªÉ ch·ªçn model m·∫∑c ƒë·ªãnh.")
    parser.add_argument("--chat", action="store_true", help="B·∫≠t ch·∫ø ƒë·ªô chat.")
    parser.add_argument("-m", "--model", type=str, default=config.get("default_model"), help="Ch·ªçn model (ghi ƒë√® t·∫°m th·ªùi).")
    parser.add_argument("-f", "--format", type=str, choices=['rich', 'raw'], default=config.get("default_format"), help="ƒê·ªãnh d·∫°ng output.")
    parser.add_argument("-rd", "--read-dir", action="store_true", help="ƒê·ªçc ng·ªØ c·∫£nh th∆∞ m·ª•c.")
    parser.add_argument("-p", "--persona", type=str, choices=list(config.get("personas", {}).keys()), help="Ch·ªçn persona.")
    parser.add_argument("-si", "--system-instruction", type=str, default=None, help="Ghi ƒë√® ch·ªâ d·∫´n h·ªá th·ªëng.")
    parser.add_argument("--load", type=str, help="T·∫£i l·ªãch s·ª≠ chat.")
    parser.add_argument("--save", type=str, help="L∆∞u l·ªãch s·ª≠ chat v√†o file c·ª• th·ªÉ.")
    parser.add_argument("-i", "--image", type=str, help="ƒê∆∞·ªùng d·∫´n t·ªõi file ·∫£nh ƒë·ªÉ ph√¢n t√≠ch.")
    parser.add_argument("--history", action="store_true", help="Hi·ªÉn th·ªã tr√¨nh duy·ªát l·ªãch s·ª≠ chat.")
    parser.add_argument("--git-commit", action="store_true", help="T·ª± ƒë·ªông t·∫°o commit message.")
    parser.add_argument("--document", type=str, metavar="FILE_PATH", help="T·ª± ƒë·ªông vi·∫øt t√†i li·ªáu cho code trong file.")
    parser.add_argument("--refactor", type=str, metavar="FILE_PATH", help="ƒê·ªÅ xu·∫•t c√°ch t√°i c·∫•u tr√∫c code trong file.")

    args = parser.parse_args()
    api_key = os.getenv("GOOGLE_API_KEY")

    if not api_key:
        console.print("[bold red]L·ªói: Vui l√≤ng thi·∫øt l·∫≠p GOOGLE_API_KEY trong file .env[/bold red]")
        return
    
    try:
        api.configure_api(api_key)
        if args.list_models:
            api.list_models(console)
            return
        if args.set_model:
            model_selection_wizard(console, config)
            return
        if args.history:
            show_history_browser(console)
            return
        if args.document or args.refactor:
            handle_code_helper(args, config, console)
            return
        system_instruction = args.system_instruction or \
                             (config.get("personas", {}).get(args.persona) if args.persona else None) or \
                             config.get("default_system_instruction")
        history = None
        if args.load:
            try:
                with open(args.load, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                console.print(f"[green]ƒê√£ t·∫£i l·ªãch s·ª≠ t·ª´ '{args.load}'.[/green]")
            except Exception as e:
                console.print(f"[bold red]L·ªói khi t·∫£i l·ªãch s·ª≠: {e}[/bold red]")
                return
        if args.model != config.get("default_model"):
            models_to_try = [args.model]
        else:
            models_to_try = config.get("model_fallback_order", [config.get("default_model")])
        response = None
        if args.chat:
            chat_session = api.start_chat_session(models_to_try[0], system_instruction, history)
            run_chat_mode(chat_session, console, config, args.format, args.save)
            return
        piped_input = None
        if not sys.stdin.isatty():
             piped_input = sys.stdin.read().strip()
        if not args.prompt and not piped_input and not args.image and not args.git_commit:
             console.print("[bold red]L·ªói: C·∫ßn cung c·∫•p prompt ho·∫∑c m·ªôt h√†nh ƒë·ªông nh∆∞ --git-commit, --chat, etc.[/bold red]")
             parser.print_help()
             return
        prompt_parts = []
        user_question = args.prompt or ""
        if args.image:
            try:
                img = Image.open(args.image)
                prompt_parts.append(img)
            except FileNotFoundError:
                console.print(f"[bold red]L·ªói: Kh√¥ng t√¨m th·∫•y file ·∫£nh '{args.image}'[/bold red]")
                return
            except Exception as e:
                console.print(f"[bold red]L·ªói khi m·ªü ·∫£nh: {e}[/bold red]")
                return
        prompt_text = ""
        if piped_input:
            prompt_text += f"D·ª±a v√†o n·ªôi dung ƒë∆∞·ª£c cung c·∫•p sau ƒë√¢y:\n{piped_input}\n\n"
        if args.read_dir:
            console.print("[yellow]ƒêang ƒë·ªçc ng·ªØ c·∫£nh th∆∞ m·ª•c...[/yellow]")
            context = utils.get_directory_context()
            prompt_text += f"D·ª±a v√†o ng·ªØ c·∫£nh c√°c file d∆∞·ªõi ƒë√¢y:\n{context}\n\n"
        if args.image:
             prompt_text += user_question if user_question else "Ph√¢n t√≠ch ·∫£nh n√†y."
        else:
             prompt_text += user_question
        if prompt_text:
            prompt_parts.append(prompt_text)
        for model in models_to_try:
            try:
                if len(models_to_try) > 1:
                    console.print(f"[dim]ƒêang th·ª≠ v·ªõi model: {model}...[/dim]")
                chat_session = api.start_chat_session(model, system_instruction, history)
                if args.git_commit:
                    handle_git_commit(chat_session, console, args.format)
                    return
                with console.status(f"[bold green]AI (model: {model}) ƒëang suy nghƒ©...[/bold green]"):
                    response = api.send_message(chat_session, prompt_parts)
                break
            except ResourceExhausted:
                console.print(f"[bold yellow]‚ö†Ô∏è Model '{model}' ƒë√£ h·∫øt h·∫°n ng·∫°ch.[/bold yellow]")
                if model == models_to_try[-1]:
                    console.print("[bold red]‚ùå ƒê√£ th·ª≠ h·∫øt c√°c model d·ª± ph√≤ng nh∆∞ng ƒë·ªÅu th·∫•t b·∫°i.[/bold red]")
                else:
                    console.print("[cyan]ƒêang t·ª± ƒë·ªông chuy·ªÉn sang model ti·∫øp theo...[/cyan]")
                continue
            except Exception as e:
                console.print(f"[bold red]ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën v·ªõi model {model}: {e}[/bold red]")
                break
        if response:
            console.print("\nüí° [bold green]Ph·∫£n h·ªìi:[/bold green]")
            response_text = get_response_text(response)
            if args.format == 'rich':
                console.print(Markdown(response_text))
            else:
                console.print(response_text)
            if response.usage_metadata:
                tokens = response.usage_metadata.total_token_count
                console.print(f"\n[dim]Token usage: {tokens}[/dim]")
            utils.execute_suggested_commands(response_text, console)
    except KeyboardInterrupt:
        console.print("\n[yellow]ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng.[/yellow]")
    except Exception as e:
        console.print(f"[bold red]ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}[/bold red]")
        console.print(f"[dim]{traceback.format_exc()}[/dim]")

if __name__ == "__main__":
    main()